{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gsMrWT3u4WM"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Install the intersection example.\n",
    "\n",
    "**Note**: The runtime needs to be restarted after installing the dependencies, hence `os.kill(os.getpid(), 9)` is added to stop the current session. Please ignore any resulting error message and simply continue to execute the subsequent cells as per normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!git clone https://github.com/huawei-noah/SMARTS /content/SMARTS\n",
    "!cd /content/SMARTS && git checkout intersection-v0 && cd /content/SMARTS/examples/rl/intersection && pip install --force-reinstall .\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/content/SMARTS/\")\n",
    "import os\n",
    "\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop reinforcement learning code\n",
    "\n",
    "We begin by building the necessary environment wrappers. Firstly, an info wrapper is built to help log instances when the ego agent succesfully completes an unprotected left turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "class Info(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super(Info, self).__init__(env)\n",
    "\n",
    "    def step(self, action: Any) -> Tuple[Any, float, bool, Dict[str, Any]]:\n",
    "        \"\"\"Steps the environment. A new \"is_success\" key is added to the\n",
    "        returned `info`.\n",
    "\n",
    "        Args:\n",
    "            action (Any): Action for the agent.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[ Any, float, bool, Dict[str, Any] ]:\n",
    "                Observation, reward, done, and info, for the agent is returned.\n",
    "        \"\"\"\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        info[\"is_success\"] = bool(info[\"score\"])\n",
    "\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `intersection-v0` environment has a continuous action space of `gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)` described by\n",
    "+ Throttle: [0,1]\n",
    "+ Brake: [0,1]\n",
    "+ Steering: [-1,1]\n",
    "\n",
    "In order to build a simple reinforcement learning policy, we discretise the action space using an action wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Action(gym.ActionWrapper):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "        self._wrapper, self.action_space = _discrete()\n",
    "\n",
    "    def action(self, action):\n",
    "        \"\"\"Adapts the action input to the wrapped environment.\n",
    "\n",
    "        Note: Users should not directly call this method.\n",
    "        \"\"\"\n",
    "        wrapped_act = self._wrapper(action)\n",
    "        return wrapped_act\n",
    "\n",
    "\n",
    "def _discrete() -> Tuple[Callable[[int], np.ndarray], gym.Space]:\n",
    "    space = gym.spaces.Discrete(n=4)\n",
    "\n",
    "    action_map = {\n",
    "        # key: [throttle, brake, steering]\n",
    "        0: [0.3, 0, 0],  # keep_direction\n",
    "        1: [0, 1, 0],  # slow_down\n",
    "        2: [0.3, 0, -0.5],  # turn_left\n",
    "        3: [0.3, 0, 0.5],  # turn_right\n",
    "    }\n",
    "\n",
    "    def wrapper(model_action: int) -> np.ndarray:\n",
    "        throttle, brake, steering = action_map[model_action]\n",
    "        return np.array([throttle, brake, steering], dtype=np.float32)\n",
    "\n",
    "    return wrapper, space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the rewards using a reward wrapper. The agent is rewarded based on the distance travelled (in meters) per step and is penalised when it collides, goes off-road, goes off-route, goes wrong-way, or drives on the road shoulder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Reward(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Adapts the wrapped environment's step.\n",
    "\n",
    "        Note: Users should not directly call this method.\n",
    "        \"\"\"\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        wrapped_reward = self._reward(obs, reward)\n",
    "\n",
    "        if done:\n",
    "            if obs[\"events\"][\"reached_goal\"]:\n",
    "                print(f\"ENV: Hooray! Vehicle reached goal.\")\n",
    "            elif obs[\"events\"][\"reached_max_episode_steps\"]:\n",
    "                print(f\"ENV: Vehicle reached max episode steps.\")\n",
    "            elif (\n",
    "                obs[\"events\"][\"off_road\"]\n",
    "                | obs[\"events\"][\"collisions\"]\n",
    "                | obs[\"events\"][\"off_route\"]\n",
    "                | obs[\"events\"][\"on_shoulder\"]\n",
    "                | obs[\"events\"][\"wrong_way\"]\n",
    "            ):\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Events: \", obs[\"events\"])\n",
    "                raise Exception(\"Episode ended for unknown reason.\")\n",
    "\n",
    "        return obs, wrapped_reward, done, info\n",
    "\n",
    "    def _reward(self, obs: Dict[str, gym.Space], env_reward: np.float64) -> np.float64:\n",
    "        reward = 0\n",
    "\n",
    "        # Penalty for driving off road\n",
    "        if obs[\"events\"][\"off_road\"]:\n",
    "            reward -= 10\n",
    "            print(f\"ENV: Vehicle went off road.\")\n",
    "            return np.float64(reward)\n",
    "\n",
    "        # Penalty for driving on road shoulder\n",
    "        if obs[\"events\"][\"on_shoulder\"]:\n",
    "            reward -= 10\n",
    "            print(f\"ENV: Vehicle went on road shoulder.\")\n",
    "            return np.float64(reward)\n",
    "\n",
    "        # Penalty for driving on wrong way\n",
    "        if obs[\"events\"][\"wrong_way\"]:\n",
    "            reward -= 10\n",
    "            print(f\"ENV: Vehicle went wrong way.\")\n",
    "            return np.float64(reward)\n",
    "\n",
    "        # Penalty for colliding\n",
    "        if obs[\"events\"][\"collisions\"]:\n",
    "            reward -= 10\n",
    "            print(f\"ENV: Vehicle collided.\")\n",
    "            return np.float64(reward)\n",
    "\n",
    "        # Penalty for driving off route\n",
    "        if obs[\"events\"][\"off_route\"]:\n",
    "            reward -= 10\n",
    "            print(f\"ENV: Vehicle went off route.\")\n",
    "            return np.float64(reward)\n",
    "\n",
    "        # Reward for distance travelled\n",
    "        reward += env_reward\n",
    "\n",
    "        return np.float64(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space of `intersection-v0` environment is described in `SMARTS/smarts/env/intersection_env.py`. \n",
    "\n",
    "In this tutorial, only the top-down rgb image from the observation space is used as input to our reinforcement learning policy. Therefore, a wrapper is built to filter the top-down rgb image observation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Observation(gym.ObservationWrapper):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "        old_space = env.observation_space[\"top_down_rgb\"]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(old_space.shape[-1],) + old_space.shape[:-1],\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    def observation(self, obs: Dict[str, gym.Space]) -> np.ndarray:\n",
    "        top_down_rgb = obs[\"top_down_rgb\"]\n",
    "\n",
    "        # Channel first\n",
    "        top_down_rgb = top_down_rgb.transpose(2, 0, 1)\n",
    "\n",
    "        return np.uint8(top_down_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to make the `intersection-v0` environment and wrap it with the previously built wrappers. The environment is additionally wrapped with `VecFrameStack`, from Stable Baselines3 library, to give a sense of time to the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
    "\n",
    "\n",
    "def make_env(config: Dict[str, Any]) -> gym.Env:\n",
    "    # Create environment\n",
    "    env = gym.make(\n",
    "        \"smarts.env:intersection-v0\",\n",
    "        headless=True,\n",
    "        visdom=False,\n",
    "        sumo_headless=True,\n",
    "        img_meters=config[\"img_meters\"],\n",
    "        img_pixels=config[\"img_pixels\"],\n",
    "    )\n",
    "\n",
    "    # Wrap env with action, reward, and observation wrapper\n",
    "    env = Info(env=env)\n",
    "    env = Action(env=env)\n",
    "    env = Reward(env=env)\n",
    "    env = Observation(env=env)\n",
    "\n",
    "    # Check custom environment\n",
    "    check_env(env)\n",
    "\n",
    "    # Wrap env with SB3 wrappers\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(venv=env, n_stack=config[\"n_stack\"], channels_order=\"first\")\n",
    "    env = VecMonitor(\n",
    "        venv=env,\n",
    "        filename=str(config[\"logdir\"]),\n",
    "        info_keywords=(\"is_success\",),\n",
    "    )\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v0xcpPfv6L3"
   },
   "source": [
    "The training and evaluation code is prepared using the Stable Baseline3 API. Training progress is checkpointed using a callback. At the end, the trained agent is saved and evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWsQgj0NvGNt"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any, Dict\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb3lib\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "def run(env: gym.Env, eval_env: gym.Env, config: Dict[str, Any]):\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=config[\"checkpoint_freq\"],\n",
    "        save_path=config[\"logdir\"] / \"checkpoint\",\n",
    "        name_prefix=config[\"alg\"],\n",
    "    )\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env=eval_env,\n",
    "        n_eval_episodes=config[\"eval_eps\"],\n",
    "        eval_freq=config[\"eval_freq\"],\n",
    "        log_path=config[\"logdir\"] / \"eval\",\n",
    "        best_model_save_path=config[\"logdir\"] / \"eval\",\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    if config[\"mode\"] == \"evaluate\":\n",
    "        print(\"\\nStart evaluation.\\n\")\n",
    "        model = getattr(sb3lib, config[\"alg\"]).load(\n",
    "            config[\"model\"], print_system_info=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nStart training from scratch.\\n\")\n",
    "        model = getattr(sb3lib, config[\"alg\"])(\n",
    "            env=env,\n",
    "            verbose=1,\n",
    "            tensorboard_log=config[\"logdir\"] / \"tensorboard\",\n",
    "            **config[\"alg_kwargs\"],\n",
    "        )\n",
    "        model.learn(\n",
    "            total_timesteps=config[\"train_steps\"],\n",
    "            callback=[checkpoint_callback, eval_callback],\n",
    "        )\n",
    "        save_dir = config[\"logdir\"] / \"train\"\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        model.save(save_dir / (\"model_\" + time))\n",
    "        print(\"\\nSaved trained model.\\n\")\n",
    "\n",
    "    print(\"\\nEvaluate policy.\\n\")\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, eval_env, n_eval_episodes=config[\"eval_eps\"], deterministic=True\n",
    "    )\n",
    "    print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    print(\"\\nFinished evaluating.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify several training prameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ResourceWarning)\n",
    "\n",
    "config = {\n",
    "    \"img_meters\": 50,  # Observation image area size in meters.\n",
    "    \"img_pixels\": 112,  # Observation image size in pixels.\n",
    "    \"n_stack\": 3,  # Number of frames to stack as input to policy network.\n",
    "    \"train_steps\": 2500,  # Number of training steps.\n",
    "    \"checkpoint_freq\": 1e3,  # Save a model every checkpoint_freq calls to env.step().\n",
    "    \"eval_eps\": 10,  # Number of evaluation epsiodes.\n",
    "    \"eval_freq\": 1e3,  # Evaluate the trained model every eval_freq steps and save the best model.\n",
    "    \"alg\": \"PPO\",  # Stable Baselines3 algorithm.\n",
    "    \"alg_kwargs\": {\"policy\": \"CnnPolicy\", \"target_kl\": 0.1},  # Network policy.\n",
    "}\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    # Make training and evaluation environments.\n",
    "    env = make_env(config=config)\n",
    "    eval_env = make_env(config=config)\n",
    "\n",
    "    # Run training or evaluation.\n",
    "    run(env=env, eval_env=eval_env, config=config)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "logdir = Path(\"/content/SMARTS/examples/rl/intersection/logs\") / time\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"logdir\"] = logdir\n",
    "config[\"mode\"] = \"train\"\n",
    "\n",
    "# Train the agent\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a pre-trained agent\n",
    "\n",
    "Evaluate a pre-trained agent and compare its performance with the newly-trained agent above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained agent.\n",
    "!curl -o /content/SMARTS/examples/rl/intersection/logs/pretrained/intersection.zip --create-dirs -L https://github.com/Adaickalavan/SMARTS-zoo/raw/main/intersection-v0/PPO_5800000_steps.zip\n",
    "\n",
    "time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "logdir = Path(\"/content/SMARTS/examples/rl/intersection/logs\") / time\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"logdir\"] = logdir\n",
    "config[\"mode\"] = \"evaluate\"\n",
    "config[\n",
    "    \"model\"\n",
    "] = \"/content/SMARTS/examples/rl/intersection/logs/pretrained/intersection\"\n",
    "\n",
    "# Evaluate the pre-trained agent\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "For reference, you may want to view the training logs captured during the training of the pre-trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tensorboard logs of the pre-trained agent\n",
    "!curl -o /content/SMARTS/examples/rl/intersection/logs/pretrained/events.out.tfevents.1651601587.gx3.16.0 --create-dirs -L https://github.com/Adaickalavan/SMARTS-zoo/raw/main/intersection-v0/events.out.tfevents.1651601587.gx3.16.0\n",
    "\n",
    "# Load the TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/SMARTS/examples/rl/intersection/logs/pretrained/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sb3_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
