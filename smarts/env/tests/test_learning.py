# MIT License
#
# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
"""This is a (long-running) regression test to ensure code changes have not impacted
learning. It compares the mean episode reward of a newly trained agent with that
of a baseline from a past (baseline) commit. It is set to run for one hour.
"""
import multiprocessing
from pathlib import Path

from ray import tune
from ray.rllib.models import ModelCatalog

from examples.rllib_agent import TrainingModel, rllib_agent
from smarts.core.utils.file import make_dir_in_smarts_log_dir
from smarts.env.rllib_hiway_env import RLlibHiWayEnv

HORIZON = 5000

ModelCatalog.register_custom_model(TrainingModel.NAME, TrainingModel)


def test_learning_regression_rllib():
    rllib_policies = {
        "policy": (
            None,
            rllib_agent["observation_space"],
            rllib_agent["action_space"],
            {"model": {"custom_model": TrainingModel.NAME}},
        )
    }

    # XXX: We should be able to simply provide "scenarios/loop"?
    scenario_path = Path(__file__).parent / "../../../scenarios/loop"
    scenario_path = str(scenario_path.absolute())

    tune_confg = {
        "env": RLlibHiWayEnv,
        "env_config": {
            "scenarios": [scenario_path],
            "seed": 42,
            "headless": True,
            "agent_specs": {"Agent-007": rllib_agent["agent_spec"]},
        },
        "multiagent": {
            "policies": rllib_policies,
            "policy_mapping_fn": lambda _: "policy",
        },
        "log_level": "WARN",
        "num_workers": multiprocessing.cpu_count() - 1,
        "horizon": HORIZON,
    }

    analysis = tune.run(
        "PPO",
        name="learning_regression_test",
        stop={"training_iteration": 60},
        max_failures=10,
        local_dir=make_dir_in_smarts_log_dir("smarts_learning_regression"),
        config=tune_confg,
    )

    df = analysis.dataframe()

    # Lower-bound 95% confidence interval of mean reward after one hour, generated by manual analysis.
    # If you need to update this, run tools/regression_rllib.py.
    ci95_reward_file = Path(__file__).parent / "ci95_reward_lo"
    with open(ci95_reward_file.absolute()) as f:
        CI95_REWARD_MEAN_1_HOUR = float(f.readline())

    assert (
        df["episode_reward_mean"][0] >= CI95_REWARD_MEAN_1_HOUR
    ), "Mean reward did not reach the expected value ({} < {})".format(
        df["episode_reward_mean"][0], CI95_REWARD_MEAN_1_HOUR
    )
